{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gsE5rhFuCiQj"
      },
      "outputs": [],
      "source": [
        "# Ref:\n",
        "# YouTube Video\n",
        "# Sam Witteveen\n",
        "# Creating an AI Agent with LangGraph Llama 3 & Groq\n",
        "# https://www.youtube.com/watch?v=lvQ96Ssesfk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install groq\n",
        "!pip -q install tavily-python\n",
        "!pip -q install -U langchain langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmd7h1G1c44M",
        "outputId": "5198a161-a54d-4c6e-b1a5-6c70f8d9a955"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langgraph"
      ],
      "metadata": {
        "id": "gouyDKMkCo60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4747ec-aa9f-449d-c905-95fe5123c4b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langgraph\n",
            "Version: 0.0.66\n",
            "Summary: langgraph\n",
            "Home-page: https://www.github.com/langchain-ai/langgraph\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: langchain-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRd5X-eQC6hp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "ygdw7ujTCo3w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set API keys as environment variables"
      ],
      "metadata": {
        "id": "ByPSsfmfHK51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "#os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "HOS4GTwOCo1H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the API clients"
      ],
      "metadata": {
        "id": "Cqxrj_T6C_z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from tavily import TavilyClient\n",
        "\n",
        "groq_client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")\n",
        "\n",
        "tavily_client = TavilyClient(api_key=userdata.get('TAVILY_API_KEY'))\n"
      ],
      "metadata": {
        "id": "PzAyFOnWC_d9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the objective?\n",
        "\n",
        "The objective is to answer a multi-part question from a user.\n",
        "\n",
        "Use a keyword creation agent instead of a ReAct pattern to answer user questions.<br>\n",
        "The input is a user question<br>\n",
        "The output is the answer to the user question."
      ],
      "metadata": {
        "id": "2Ox3Pq34Eq8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draw the graph\n",
        "- Give each function a number\n",
        "- Give each edge a number"
      ],
      "metadata": {
        "id": "WCjgqtaA6CL5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXpFe86y6DOF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List the inputs and graph functions\n",
        "Each point in the graph is just a function.<br>\n",
        "There are node functions and conditional edge functions.<br>\n",
        "The inputs are passed to the graph at the start (as a dict).<br>\n",
        "They automatically initialize the values in the state.\n",
        "\n",
        "Inputs\n",
        "- keywords_llm_2_system_message\n",
        "- final_answer_llm_4_system_message\n",
        "- user_query\n",
        "- num_steps\n",
        "\n",
        "Functions\n",
        "1. query_keywords_llm_1 (node) (llm)\n",
        "2. run_web_search_2 (node) (tavily)\n",
        "3. query_final_answer_llm_3 (node) (llm)\n",
        "4. save_final_answer_4 (node)\n",
        "5. print_the_state_5 (node)"
      ],
      "metadata": {
        "id": "a8XsnWSc9erx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "LwW76J3yJ5QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_markdown_file(content, filename):\n",
        "  \"\"\"Writes the given content as a markdown file to the local directory.\n",
        "\n",
        "  Args:\n",
        "    content: The string content to write to the file.\n",
        "    filename: The filename to save the file as.\n",
        "  \"\"\"\n",
        "  with open(f\"{filename}.md\", \"w\") as f:\n",
        "    f.write(content)\n"
      ],
      "metadata": {
        "id": "W3YQhpBeJ69J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_message_history(system_message, user_input):\n",
        "\n",
        "    \"\"\"\n",
        "    Create a message history messages list.\n",
        "    Args:\n",
        "        system_message (str): The system message\n",
        "        user_query (str): The user input\n",
        "    Returns:\n",
        "        A list of dicts in OpenAi chat format\n",
        "    \"\"\"\n",
        "\n",
        "    message_history = [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": system_message\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": user_input\n",
        "                        }\n",
        "                    ]\n",
        "\n",
        "    return message_history\n",
        "\n"
      ],
      "metadata": {
        "id": "sLjDylVf-amy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the LLM"
      ],
      "metadata": {
        "id": "gaR_IGtcfXgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_llm_api_call(message_history):\n",
        "\n",
        "    \"\"\"\n",
        "    Makes a call to the Llama3 model on Groq.\n",
        "    Args:\n",
        "        message_history (List of dicts): The message history\n",
        "    Returns:\n",
        "        response_text: (str): The text response from the LLM\n",
        "    \"\"\"\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "                        messages=message_history,\n",
        "                        model=\"llama3-70b-8192\",\n",
        "                    )\n",
        "\n",
        "    response_text = response.choices[0].message.content\n",
        "\n",
        "    return response_text\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "system_message = \"Your name is Molly.\"\n",
        "user_message = \"What's your name?\"\n",
        "\n",
        "message_history = create_message_history(system_message, user_message)\n",
        "\n",
        "response = make_llm_api_call(message_history)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ETAFEcNBnBD",
        "outputId": "1acdc4af-7efe-4066-ab07-2ef179533fb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Molly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the tools"
      ],
      "metadata": {
        "id": "I8HhzLhjfCJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tavily_search(query, num_results=5):\n",
        "\n",
        "    \"\"\"\n",
        "    Uses the Tavily API to run a web search\n",
        "    Args:\n",
        "        query (str): The user query\n",
        "        num_results (int): Num search results\n",
        "    Returns:\n",
        "        tav_response (json string): The search results in json format\n",
        "    \"\"\"\n",
        "\n",
        "    # For basic search:\n",
        "    tav_response = tavily_client.search(query=query, max_results=num_results)\n",
        "\n",
        "    return tav_response\n",
        "\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "query = \"How much does a bulldog weigh?\"\n",
        "\n",
        "results = run_tavily_search(query, num_results=2)\n",
        "\n",
        "# Use this str output in the system message example below\n",
        "# Use this instead of the Eisenhower example\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWpzGxpSfFKX",
        "outputId": "f64eb6cf-f95f-4fa9-8cbb-d0bfec211866"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'How much does a bulldog weigh?', 'follow_up_questions': None, 'answer': None, 'images': None, 'results': [{'title': 'English Bulldog Growth & Weight Chart: Everything You Need To ... - Pawlicy', 'url': 'https://www.pawlicy.com/blog/english-bulldog-growth-and-weight-chart/', 'content': 'According to Care.com, puppies reach about 75% of their adult height at six months old. This will be around 10-13 inches tall for a male English Bulldog and approximately 9-11 inches tall for a female English Bulldog. As for weight, a 6-month-old male English Bulldog will weigh about 33 to 37 pounds, while a 6-month-old female English Bulldog ...', 'score': 0.94611, 'raw_content': None}, {'title': 'English Bulldog Growth and Weight Chart (Male & Female)', 'url': 'https://www.k9web.com/breeds/english-bulldog-growth-chart/', 'content': 'Male two-month-old Bulldogs will weigh between 9 and 12 pounds (4 and 5.4 kg), while females should weigh 7 and 10 pounds (3.1 and 4.5 kg). ... If your dog seems to be putting on too much weight too quickly, you may consider taking him to the vet to rule out common health problems such as hypothyroidism, leading to excessive weight gain. 2 ...', 'score': 0.93715, 'raw_content': None}], 'response_time': 1.26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the system messages\n",
        "\n",
        "Set up the system message and test the performance of the LLM."
      ],
      "metadata": {
        "id": "ATO_-NVygqBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords_system_message = \"\"\"\n",
        "You are a master at working out the best keywords and phrases to search for in a web search to get the best info for the customer.\n",
        "\n",
        "You will be given a USER_QUERY. Work out the best search keywords that will find the best\n",
        "info for helping to answer the user query.\n",
        "\n",
        "Return a JSON with a single key 'keywords' with a list of no more than 5 keywords or phrases, and no premable or explaination.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "user_message = \"What are the currencies of South Africa and Thailand?\"\n",
        "\n",
        "user_message = \"What was Dwight Eisenhowers presidential campaign slogan?\"\n",
        "\n",
        "message_history = create_message_history(keywords_system_message, user_message)\n",
        "\n",
        "response = make_llm_api_call(message_history)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRApwjAxKQ4B",
        "outputId": "725f3625-fe22-4e51-f058-9e318908bbe6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ \"keywords\" : [\"Dwight Eisenhower\", \"presidential campaign\", \"slogan\", \"I Like Ike\", \"Eisenhower campaign history\"] }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_answer_system_message = \"\"\"\n",
        "You are the Final Answer Agent. Take the USER_QUERY and the RESEARCH_INFO from the research agent and \\\n",
        "write a helpful response in a thoughtful and friendly way.\n",
        "\n",
        "You never make up information that hasn't been provided in the research_info.\n",
        "\n",
        "Return the response as a JSON string with a single key 'final_answer' and no premable or explaination.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "# Get the search keywords\n",
        "user_query = \"What is the currency of South Africa and Thailand?\"\n",
        "message_history = create_message_history(keywords_system_message, user_query)\n",
        "\n",
        "response = make_llm_api_call(message_history)\n",
        "\n",
        "response = json.loads(response)\n",
        "keywords_list = (response['keywords'])\n",
        "\n",
        "# Run searches using the keywords\n",
        "research_info_list = []\n",
        "\n",
        "for query in keywords_list:\n",
        "\n",
        "    results = run_tavily_search(query, num_results=2)\n",
        "    research_info_list.append(results)\n",
        "\n",
        "\n",
        "# Get the final answer\n",
        "\n",
        "input = f\"\"\"\n",
        "USER_QUERY: {user_query}\n",
        "RESEARCH_INFO: {research_info_list}\n",
        "\"\"\"\n",
        "\n",
        "message_history = create_message_history(final_answer_system_message, input)\n",
        "\n",
        "response = make_llm_api_call(message_history)\n",
        "\n",
        "response = json.loads(response)\n",
        "final_answer = response['final_answer']\n",
        "\n",
        "final_answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_CgLuPQWKQ0p",
        "outputId": "5ee0f512-0fb9-4179-cbdd-ad5d23b9963f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The currency of South Africa is the South African Rand (ZAR) and the currency of Thailand is the Thai Baht (THB).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MU5HR5otSzZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the inputs\n",
        "These inputs are passed into the graph at the start.<br>\n",
        "They initialize the variables in the state.\n",
        "\n",
        "Inputs\n",
        "- keywords_system_message\n",
        "- final_answer_system_message\n",
        "- user_query\n",
        "- research_info_list\n",
        "- num_steps"
      ],
      "metadata": {
        "id": "n5MBz582Vjk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs to initialize state variables\n",
        "\n",
        "# inputs = {\"keywords_system_message\": keywords_system_message,\n",
        "# \"final_answer_system_message\": final_answer_system_message,\n",
        "# \"user_query\": user_query,\n",
        "# \"num_steps\": 0}"
      ],
      "metadata": {
        "id": "DJEGu9boVmdh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the graph functions\n",
        "Each point in the graph is just a function.<br>\n",
        "There are node functions and conditional edge functions.\n",
        "\n",
        "Functions\n",
        "1. query_keywords_llm_1 (node) (llm)\n",
        "2. run_web_search_2 (node) (tavily)\n",
        "3. query_final_answer_llm_3 (node) (llm)\n",
        "4. save_final_answer_4 (node)\n",
        "5. print_the_state_5 (node)"
      ],
      "metadata": {
        "id": "gjS0lyW8Nec5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_keywords_llm_1(state):\n",
        "\n",
        "    print(\"---MAKE KEYWORDS LLM API CALL---\")\n",
        "\n",
        "    # Increment the steps\n",
        "    num_steps = int(state['num_steps'])\n",
        "    num_steps += 1\n",
        "\n",
        "    user_query = state['user_query']\n",
        "    keywords_system_message = state['keywords_system_message']\n",
        "\n",
        "    message_history = create_message_history(keywords_system_message, user_query)\n",
        "\n",
        "    response = make_llm_api_call(message_history)\n",
        "\n",
        "    response = json.loads(response)\n",
        "    keywords_list = response['keywords']\n",
        "\n",
        "    print('Keyword generation complete.')\n",
        "    print(keywords_list)\n",
        "\n",
        "    # Update the state\n",
        "    return {\"keywords_list\": keywords_list, \"num_steps\": num_steps}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N-NbHrlOMK17"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_web_search_2(state):\n",
        "\n",
        "    print(\"---RUN WEB SEARCHES - TAVILY---\")\n",
        "\n",
        "    # Increment the steps\n",
        "    num_steps = int(state['num_steps'])\n",
        "    num_steps += 1\n",
        "\n",
        "    keywords_list = state[\"keywords_list\"]\n",
        "\n",
        "    print(type(keywords_list))\n",
        "\n",
        "    print(\"Search keywords:\", keywords_list)\n",
        "\n",
        "    # Run searches using the keywords\n",
        "    research_info_list = []\n",
        "\n",
        "    for query in keywords_list:\n",
        "\n",
        "        results = run_tavily_search(query, num_results=2)\n",
        "        research_info_list.append(results)\n",
        "\n",
        "    print(\"Web research complete.\")\n",
        "\n",
        "    # Update the state\n",
        "    return {\"research_info_list\": research_info_list, \"num_steps\": num_steps}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zmt-kDfAYNQr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_final_answer_llm_3(state):\n",
        "\n",
        "    print(\"---MAKE FINAL ANSWER LLM API CALL---\")\n",
        "\n",
        "    # Increment the steps\n",
        "    num_steps = int(state['num_steps'])\n",
        "    num_steps += 1\n",
        "\n",
        "    user_query = state['user_query']\n",
        "    final_answer_system_message = state['final_answer_system_message']\n",
        "    research_info_list = state[\"research_info_list\"]\n",
        "\n",
        "\n",
        "    input = f\"\"\"\n",
        "    USER_QUERY: {user_query}\n",
        "    RESEARCH_INFO: {research_info_list}\n",
        "    \"\"\"\n",
        "\n",
        "    message_history = create_message_history(final_answer_system_message, input)\n",
        "\n",
        "    response = make_llm_api_call(message_history)\n",
        "\n",
        "    print(response)\n",
        "\n",
        "    response = json.loads(response)\n",
        "    final_answer = response['final_answer']\n",
        "\n",
        "\n",
        "    # Update the state\n",
        "    return {\"final_answer\": final_answer, \"num_steps\": num_steps}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S86VPWxyYNMf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_final_answer_4(state):\n",
        "\n",
        "    print(\"---SAVING FINAL ANSWER---\")\n",
        "    num_steps = state['num_steps']\n",
        "    num_steps += 1\n",
        "\n",
        "    # Extract the final answer\n",
        "    final_answer = state[\"final_answer\"]\n",
        "\n",
        "\n",
        "    print(\"Final answer:\", final_answer)\n",
        "\n",
        "    # Save the answer to a file\n",
        "    write_markdown_file(str(final_answer), \"final_answer\")\n",
        "\n",
        "    print(\"Final answer saved to a file.\")\n",
        "\n",
        "    # Update the state\n",
        "    return {\"num_steps\":num_steps}"
      ],
      "metadata": {
        "id": "Wh1oJMBac2D6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_the_state_5(state):\n",
        "\n",
        "    \"\"\"\n",
        "    Print the state\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---STATE PRINTER---\")\n",
        "    print(f\"Final answer: {state['final_answer']} \\n\" )\n",
        "    print(f\"Num steps: {state['num_steps']} \\n\")\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "tqLuANkiFUBA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the state\n",
        "\n",
        "The functions take the state as input.<br>\n",
        "The functions usually return a dict that automatically updates the state.<br>\n",
        "Therefore, the variables in the state need to correspond to variables that the functions output."
      ],
      "metadata": {
        "id": "4Yo-2d-KIf-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langgraph.graph import END, StateGraph"
      ],
      "metadata": {
        "id": "8bjIb61VIhFb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "### State\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the graph.\n",
        "\n",
        "    Attributes:\n",
        "        keywords_system_message: the keywords llm system message\n",
        "        final_answer_system_message: the final answer llm system message\n",
        "        user_query: the question from the user\n",
        "        keywords_list: a list of search keywords and phrases\n",
        "        research_info_list: a list of results from the Tavily web searches\n",
        "        final_answer: the final answer\n",
        "        num_steps: number of steps\n",
        "    \"\"\"\n",
        "\n",
        "    keywords_system_message : str\n",
        "    final_answer_system_message : str\n",
        "    user_query : str\n",
        "    keywords_list : List[str]\n",
        "    research_info_list : List[str]\n",
        "    final_answer : str\n",
        "    num_steps : int\n"
      ],
      "metadata": {
        "id": "DLiAcneOIm4T"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DL9aNOUKIm1X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the graph\n",
        "\n",
        "1. query_keywords_llm_1 (node) (llm)\n",
        "2. run_web_search_2 (node) (tavily)\n",
        "3. query_final_answer_llm_3 (node) (llm)\n",
        "4. save_final_answer_4 (node)\n",
        "5. print_the_state_5 (node)"
      ],
      "metadata": {
        "id": "7Fz5eK9WRWAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the graph"
      ],
      "metadata": {
        "id": "QIe_sWG87Cok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(GraphState)"
      ],
      "metadata": {
        "id": "Lx0UOcxXQK6r"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the nodes"
      ],
      "metadata": {
        "id": "fZRr0ubG66jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_node(\"query_keywords_llm_1\", query_keywords_llm_1)\n",
        "workflow.add_node(\"run_web_search_2\", run_web_search_2)\n",
        "workflow.add_node(\"query_final_answer_llm_3\", query_final_answer_llm_3)\n",
        "workflow.add_node(\"save_final_answer_4\", save_final_answer_4)\n",
        "workflow.add_node(\"print_the_state_5\", print_the_state_5)"
      ],
      "metadata": {
        "id": "4sj-cFCOQK4O"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the edges"
      ],
      "metadata": {
        "id": "lB1QItos8Gv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INPUT\n",
        "\n",
        "# e-1\n",
        "workflow.set_entry_point(\"query_keywords_llm_1\")\n",
        "\n",
        "# e-2\n",
        "workflow.add_edge(\"query_keywords_llm_1\", \"run_web_search_2\")\n",
        "\n",
        "# e-3\n",
        "workflow.add_edge(\"run_web_search_2\", \"query_final_answer_llm_3\")\n",
        "\n",
        "# e-4\n",
        "workflow.add_edge(\"query_final_answer_llm_3\", \"save_final_answer_4\")\n",
        "\n",
        "# e-5\n",
        "workflow.add_edge(\"save_final_answer_4\", \"print_the_state_5\")\n",
        "\n",
        "# e-6\n",
        "workflow.add_edge(\"print_the_state_5\", END)\n",
        "\n",
        "# END"
      ],
      "metadata": {
        "id": "UyFNQgD-MDrC"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the graph"
      ],
      "metadata": {
        "id": "Z5g9uZKLBGGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "Tfi2e7ICQKzU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the system"
      ],
      "metadata": {
        "id": "aV9i8_QuBhMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"Who was President Obama's chief of staff and who was his vice president?\"\n",
        "\n",
        "inputs = {\n",
        "            \"keywords_system_message\": keywords_system_message,\n",
        "            \"final_answer_system_message\": final_answer_system_message,\n",
        "            \"user_query\": user_query,\n",
        "            \"num_steps\": 0\n",
        "        }"
      ],
      "metadata": {
        "id": "Vmv6gJGRQKwj"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The printed outputs will be displayed when this cell is run\n",
        "\n",
        "output = app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqh3Ye3eB8mr",
        "outputId": "443ce6e1-6713-41d6-a5c3-2c5c1c276531"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---MAKE KEYWORDS LLM API CALL---\n",
            "Keyword generation complete.\n",
            "['Barack Obama staff', \"Obama's chiefs of staff\", 'Joe Biden VP', 'Obama administration officials', 'White House staff Obama era']\n",
            "---RUN WEB SEARCHES - TAVILY---\n",
            "<class 'list'>\n",
            "Search keywords: ['Barack Obama staff', \"Obama's chiefs of staff\", 'Joe Biden VP', 'Obama administration officials', 'White House staff Obama era']\n",
            "Web research complete.\n",
            "---MAKE FINAL ANSWER LLM API CALL---\n",
            "{\"final_answer\": \"President Obama's chief of staff was Rahm Emanuel, William Daley, Jacob Lew, and Denis McDonough, serving in that order. His vice president was Joe Biden.\"}\n",
            "---SAVING FINAL ANSWER---\n",
            "Final answer: President Obama's chief of staff was Rahm Emanuel, William Daley, Jacob Lew, and Denis McDonough, serving in that order. His vice president was Joe Biden.\n",
            "Final answer saved to a file.\n",
            "---STATE PRINTER---\n",
            "Final answer: President Obama's chief of staff was Rahm Emanuel, William Daley, Jacob Lew, and Denis McDonough, serving in that order. His vice president was Joe Biden. \n",
            "\n",
            "Num steps: 4 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the final email\n",
        "\n",
        "print(output['final_answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAHcVEZaB8iz",
        "outputId": "f763f256-26be-4c83-f358-d08efa15abb9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "President Obama's chief of staff was Rahm Emanuel, William Daley, Jacob Lew, and Denis McDonough, serving in that order. His vice president was Joe Biden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehiIYBt1SGTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the markdown file has been created\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "_fe6qoK7B8bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d953f3-0211-4969-c1c0-5956f2acb454"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_answer.md  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the contents of the file\n",
        "\n",
        "!cat final_answer.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jicY5XvZaoCB",
        "outputId": "c5ff0ebd-3601-4eec-b1f5-5bc3413f5efd"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "President Obama's chief of staff was Rahm Emanuel, William Daley, Jacob Lew, and Denis McDonough, serving in that order. His vice president was Joe Biden."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6CEn3pXa0ls"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}