{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gsE5rhFuCiQj"
      },
      "outputs": [],
      "source": [
        "# Ref:\n",
        "# YouTube Video\n",
        "# Sam Witteveen\n",
        "# Creating an AI Agent with LangGraph Llama 3 & Groq\n",
        "# https://www.youtube.com/watch?v=lvQ96Ssesfk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install groq\n",
        "!pip -q install tavily-python\n",
        "!pip -q install -U langchain langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmd7h1G1c44M",
        "outputId": "d5635c7e-2e11-4889-f474-9177d443cd72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langgraph"
      ],
      "metadata": {
        "id": "gouyDKMkCo60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626c1e0c-703b-43e1-db6c-02dc83b92191"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langgraph\n",
            "Version: 0.0.66\n",
            "Summary: langgraph\n",
            "Home-page: https://www.github.com/langchain-ai/langgraph\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: langchain-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRd5X-eQC6hp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "ygdw7ujTCo3w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set API keys as environment variables"
      ],
      "metadata": {
        "id": "ByPSsfmfHK51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "#os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "HOS4GTwOCo1H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the API clients"
      ],
      "metadata": {
        "id": "Cqxrj_T6C_z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from tavily import TavilyClient\n",
        "\n",
        "groq_client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")\n",
        "\n",
        "tavily_client = TavilyClient(api_key=userdata.get('TAVILY_API_KEY'))\n"
      ],
      "metadata": {
        "id": "PzAyFOnWC_d9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the objective?\n",
        "\n",
        "Create a ReAct workflow to answer user questions.<br>\n",
        "The input is a user question<br>\n",
        "The output is the answer to the user question."
      ],
      "metadata": {
        "id": "2Ox3Pq34Eq8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draw the graph\n",
        "- Give each function a number\n",
        "- Give each edge a number"
      ],
      "metadata": {
        "id": "WCjgqtaA6CL5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXpFe86y6DOF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List the inputs and graph functions\n",
        "Each point in the graph is just a function.<br>\n",
        "There are node functions and conditional edge functions.<br>\n",
        "The inputs are passed to the graph at the start (as a dict).<br>\n",
        "They automatically initialize the values in the state.\n",
        "\n",
        "Inputs\n",
        "- system_message\n",
        "- user_query\n",
        "- num_steps\n",
        "\n",
        "Functions\n",
        "1. initialize_message_history_1 (node)\n",
        "2. query_llm_2 (node) (llm)\n",
        "3. route_to_search_or_answer_3 (cond edge)\n",
        "4. run_web_search_4 (node) (tavily)\n",
        "5. save_final_answer_5 (node)\n",
        "6. print_the_state_6 (node)"
      ],
      "metadata": {
        "id": "a8XsnWSc9erx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "LwW76J3yJ5QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_markdown_file(content, filename):\n",
        "  \"\"\"Writes the given content as a markdown file to the local directory.\n",
        "\n",
        "  Args:\n",
        "    content: The string content to write to the file.\n",
        "    filename: The filename to save the file as.\n",
        "  \"\"\"\n",
        "  with open(f\"{filename}.md\", \"w\") as f:\n",
        "    f.write(content)\n"
      ],
      "metadata": {
        "id": "W3YQhpBeJ69J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_message_history(system_message, first_user_query):\n",
        "\n",
        "    \"\"\"\n",
        "    Initialize variables in the state.\n",
        "    Args:\n",
        "        system_message (str): The ReAct system message\n",
        "        user_query (str): The user search query\n",
        "    Returns:\n",
        "        A dict that automatically updates the state.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---INITIALIZE MESSAGE HISTORY---\")\n",
        "\n",
        "    message_history = [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": system_message\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": first_user_query\n",
        "                        }\n",
        "                    ]\n",
        "\n",
        "\n",
        "    return message_history\n",
        "\n"
      ],
      "metadata": {
        "id": "sLjDylVf-amy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the LLM"
      ],
      "metadata": {
        "id": "gaR_IGtcfXgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_llm_api_call(message_history):\n",
        "\n",
        "    \"\"\"\n",
        "    Makes a call to the Llama3 model on Groq.\n",
        "    Args:\n",
        "        message_history (List of dicts): The message history\n",
        "    Returns:\n",
        "        response_text: (str): The text response from the LLM\n",
        "    \"\"\"\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "                        messages=message_history,\n",
        "                        model=\"llama3-70b-8192\",\n",
        "                    )\n",
        "\n",
        "    response_text = response.choices[0].message.content\n",
        "\n",
        "    return response_text\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "message_history = [{\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"Your name is Molly.\",\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"What's your name?\",\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "response = make_llm_api_call(message_history)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ETAFEcNBnBD",
        "outputId": "d7b953e4-510a-4fc1-cdcb-0c04102e4053"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Molly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the tools"
      ],
      "metadata": {
        "id": "I8HhzLhjfCJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tavily_search(query, num_results=5):\n",
        "\n",
        "    \"\"\"\n",
        "    Uses the Tavily API to run a web search\n",
        "    Args:\n",
        "        query (str): The user query\n",
        "        num_results (int): Num search results\n",
        "    Returns:\n",
        "        tav_response (json string): The search results in json format\n",
        "    \"\"\"\n",
        "\n",
        "    # For basic search:\n",
        "    tav_response = tavily_client.search(query=query, max_results=num_results)\n",
        "\n",
        "    return tav_response\n",
        "\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "query = \"How much does a bulldog weigh?\"\n",
        "\n",
        "results = run_tavily_search(query, num_results=2)\n",
        "\n",
        "# Use this str output in the system message example below\n",
        "# Use this instead of the Eisenhower example\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWpzGxpSfFKX",
        "outputId": "1c429873-cf47-48ea-fa99-2f029464c368"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'How much does a bulldog weigh?', 'follow_up_questions': None, 'answer': None, 'images': None, 'results': [{'title': 'English Bulldog Growth and Weight Chart (Male & Female)', 'url': 'https://www.k9web.com/breeds/english-bulldog-growth-chart/', 'content': 'Male two-month-old Bulldogs will weigh between 9 and 12 pounds (4 and 5.4 kg), while females should weigh 7 and 10 pounds (3.1 and 4.5 kg). ... If your dog seems to be putting on too much weight too quickly, you may consider taking him to the vet to rule out common health problems such as hypothyroidism, leading to excessive weight gain. 2 ...', 'score': 0.93549, 'raw_content': None}, {'title': 'English Bulldog Growth & Weight Chart: Everything You Need To ... - Pawlicy', 'url': 'https://www.pawlicy.com/blog/english-bulldog-growth-and-weight-chart/', 'content': 'According to Care.com, puppies reach about 75% of their adult height at six months old. This will be around 10-13 inches tall for a male English Bulldog and approximately 9-11 inches tall for a female English Bulldog. As for weight, a 6-month-old male English Bulldog will weigh about 33 to 37 pounds, while a 6-month-old female English Bulldog ...', 'score': 0.91907, 'raw_content': None}], 'response_time': 1.34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple ReAct example"
      ],
      "metadata": {
        "id": "ObN-i4hGHqQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReAct Example\n",
        "\n",
        "system_message = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "Output your response as a JSON string.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "average_dog_weight:\n",
        "e.g. average_dog_weight: Collie\n",
        "returns average weight of a dog when given the breed\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: How much does a Bulldog weigh?\n",
        "{\n",
        "    \"Thought\": \"I should look the dogs weight using average_dog_weight\",\n",
        "    \"Action\": \"average_dog_weight: Bulldog\".\n",
        "    \"Status\": PAUSE,\n",
        "}\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: A Bulldog weights 51 lbs\n",
        "\n",
        "You then output:\n",
        "{\n",
        "    Answer: A bulldog weights 51 lbs,\n",
        "    Status: DONE,\n",
        "}\n",
        "\"\"\".strip()\n",
        "\n",
        "user_query = \"How much does a toy poodle weigh?\"\n",
        "\n",
        "message_history1 = [{\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": system_message,\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_query,\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "message_history2 = [{\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": system_message,\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_query,\n",
        "                    },\n",
        "                   {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": '{\\n    \"Thought\": \"I should look up the average weight of a toy poodle\",\\n    \"Action\": \"average_dog_weight: Toy Poodle\",\\n    \"Status\": PAUSE,\\n}',\n",
        "                    },\n",
        "                   {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"a toy poodles average weight is 7 lbs\",\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "\n",
        "\n",
        "response = make_llm_api_call(message_history2)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQoUykNrCkbm",
        "outputId": "32bd0141-2390-451f-c5ba-80d4ddd69d1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Answer\": \"A toy poodle weighs 7 lbs\",\n",
            "    \"Status\": DONE,\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the system message\n",
        "Set up the system message and test the performance of the LLM."
      ],
      "metadata": {
        "id": "ATO_-NVygqBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "system_message = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "Output your response as a JSON string.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "run_web_search:\n",
        "e.g. run_web_search: average weight of a Collie\n",
        "returns web search info relating to Collies.\n",
        "\n",
        "Break the query down into separate questions and search only one question at a time.\n",
        "You are only allowed to search one topic at a time.\n",
        "You are allowed to make multiple web searches (but not together, only in sequence).\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: How much does a Bulldog weigh?\n",
        "{\n",
        "    \"Thought\": \"I should run a web search for the average weight of a bulldog\",\n",
        "    \"Action\": \"run_web_search: Average weight of a Bulldog\".\n",
        "    \"Status\": \"PAUSE\"\n",
        "}\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: Web search results relating to Bulldogs\n",
        "\n",
        "You then output:\n",
        "{\n",
        "    \"Thought\": \"I have all the information I need to answer the question.\",\n",
        "    \"Answer\": \"A bulldog weights 51 lbs\",\n",
        "    \"Status\": \"DONE\"\n",
        "}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "user_query = \"What was Dwight Eisenhower's presidential campaign slogan?\"\n",
        "\n",
        "#user_query = \"What are the currencies of Thailand and Indonesia?\"\n",
        "\n",
        "message_history = [{\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": system_message\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_query\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "\n",
        "response1 = make_llm_api_call(message_history)\n",
        "# Add the assistants response to the message history\n",
        "messsage = {\"role\": \"assistant\", \"content\": response1}\n",
        "message_history.append(messsage)\n",
        "\n",
        "# Extract the search text\n",
        "json_response = json.loads(response1)\n",
        "search_text = json_response['Action'].replace('run_web_search:', \"\").strip()\n",
        "\n",
        "# Extract the status\n",
        "json_response = json.loads(response1)\n",
        "status = json_response['Status']\n",
        "print(status)\n",
        "\n",
        "# Run the tavily search\n",
        "search_results = run_tavily_search(search_text, num_results=10)\n",
        "\n",
        "# Add the search results to the message history\n",
        "messsage = {\"role\": \"user\", \"content\": str(search_results)}\n",
        "message_history.append(messsage)\n",
        "\n",
        "response2 = make_llm_api_call(message_history)\n",
        "\n",
        "json_response = json.loads(response2)\n",
        "status = json_response['Status']\n",
        "print(status)\n",
        "\n",
        "print(response1)\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yadtJY7uNGMe",
        "outputId": "3fdfd914-f04c-467e-c27d-14b63f5db76a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PAUSE\n",
            "DONE\n",
            "{\n",
            "    \"Thought\": \"I'm not familiar with Dwight Eisenhower's presidential campaign slogan, I should run a web search to find the answer\",\n",
            "    \"Action\": \"run_web_search: Dwight Eisenhower presidential campaign slogan\",\n",
            "    \"Status\": \"PAUSE\"\n",
            "}\n",
            "{\n",
            "    \"Thought\": \"I have all the information I need to answer the question.\",\n",
            "    \"Answer\": \"Dwight Eisenhower's presidential campaign slogan was 'I Like Ike'.\",\n",
            "    \"Status\": \"DONE\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the inputs\n",
        "These inputs are passed into the graph at the start.<br>\n",
        "They initialize the variables in the state.\n",
        "\n",
        "Inputs\n",
        "- system_message\n",
        "- user_query\n",
        "- num_steps"
      ],
      "metadata": {
        "id": "n5MBz582Vjk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs to initialize state variables\n",
        "\n",
        "# inputs = {\"system_message\": system_message, \"user_query\": user_query, \"num_steps\": 0}"
      ],
      "metadata": {
        "id": "DJEGu9boVmdh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the graph functions\n",
        "Each point in the graph is just a function.<br>\n",
        "There are node functions and conditional edge functions.\n",
        "\n",
        "Functions\n",
        "1. initialize_message_history_1 (node)\n",
        "2. query_llm_2 (node) (llm)\n",
        "3. route_to_websearch_or_answer_3 (cond edge)\n",
        "4. run_web_search_4 (node) (tavily)\n",
        "5. save_final_answer_5 (node)\n",
        "6. print_the_state_6 (node)"
      ],
      "metadata": {
        "id": "gjS0lyW8Nec5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_message_history_1(state):\n",
        "\n",
        "    \"\"\"\n",
        "    Adds the system message and the user query\n",
        "    to the message history list.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---INITIALIZE MESSAGE HISTORY---\")\n",
        "\n",
        "    # Increment the steps\n",
        "    num_steps = int(state['num_steps'])\n",
        "    num_steps += 1\n",
        "\n",
        "    system_message = state['system_message']\n",
        "    user_query = state['user_query']\n",
        "\n",
        "    message_history = [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": system_message\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": user_query\n",
        "                        }\n",
        "                    ]\n",
        "    print(\"Added system message.\")\n",
        "    print(\"Added user query.\")\n",
        "    print(\"Message history initialized.\")\n",
        "\n",
        "    # Update the state\n",
        "    return {\"message_history\": message_history, \"num_steps\": num_steps}\n",
        "\n"
      ],
      "metadata": {
        "id": "4eyzPJ8AafLy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_llm_2(state):\n",
        "\n",
        "    print(\"---MAKE LLM API CALL---\")\n",
        "\n",
        "    # Increment the steps\n",
        "    num_steps = int(state['num_steps'])\n",
        "    num_steps += 1\n",
        "\n",
        "    # Get the message history.\n",
        "    # Note that the user serach query is already included\n",
        "    message_history = state['message_history']\n",
        "\n",
        "    print(type(message_history))\n",
        "\n",
        "    # Run the llm\n",
        "    # This returns a json string\n",
        "    response = make_llm_api_call(message_history)\n",
        "\n",
        "    print(\"Response:\", response)\n",
        "\n",
        "    # Update the message history\n",
        "    # Add the assistants response to the message history\n",
        "    messsage = {\"role\": \"assistant\", \"content\": response}\n",
        "    message_history.append(messsage)\n",
        "\n",
        "    print(\"Updated message history - assistant\")\n",
        "\n",
        "    # Update the state\n",
        "    return {\"message_history\": message_history,  \"llm2_response\": response, \"num_steps\": num_steps}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N-NbHrlOMK17"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route_to_websearch_or_answer_3(state):\n",
        "\n",
        "    \"\"\"\n",
        "    Route to web search or not.\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE TO WEB SEARCH OR ANSWER---\")\n",
        "\n",
        "    # Get the last llm node 2 response from the state\n",
        "    llm2_response = state[\"llm2_response\"]\n",
        "\n",
        "    # Extract the status\n",
        "    json_response = json.loads(llm2_response)\n",
        "    status = json_response['Status']\n",
        "    print(\"Status:\", status)\n",
        "\n",
        "\n",
        "    if status == 'PAUSE':\n",
        "        print(\"Routing to web search.\")\n",
        "        return \"to_web_search\"\n",
        "\n",
        "    elif status == 'DONE':\n",
        "        print(\"Routing to final amswer\")\n",
        "        return \"to_final_answer\""
      ],
      "metadata": {
        "id": "Orge6GUBNc7m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_web_search_4(state):\n",
        "\n",
        "    print(\"---RUNNING WEB SEARCH---\")\n",
        "    # Increment the steps\n",
        "    num_steps = int(state['num_steps'])\n",
        "    num_steps += 1\n",
        "\n",
        "    # Get the last llm node 2 response from the state\n",
        "    llm2_response = state[\"llm2_response\"]\n",
        "    message_history = state[\"message_history\"]\n",
        "\n",
        "    # Extract the search text\n",
        "    json_response = json.loads(llm2_response)\n",
        "    search_text = json_response['Action'].replace('run_web_search:', \"\").strip()\n",
        "\n",
        "    print(\"Search text:\", search_text)\n",
        "\n",
        "    # Run the tavily search\n",
        "    web_search_results = run_tavily_search(search_text, num_results=5)\n",
        "\n",
        "    # Convert the search results to a string\n",
        "    web_search_results = str(web_search_results)\n",
        "\n",
        "    # Update the message history\n",
        "    # Add the search results to the message history\n",
        "    messsage = {\"role\": \"user\", \"content\": web_search_results}\n",
        "    message_history.append(messsage)\n",
        "\n",
        "    print(\"Updated message history - user, search results\")\n",
        "\n",
        "    # Update the state\n",
        "    return {\"message_history\": message_history, \"num_steps\":num_steps}"
      ],
      "metadata": {
        "id": "RXMCDnQnc2GX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_final_answer_5(state):\n",
        "\n",
        "    print(\"---EXTRACTING AND SAVING FINAL ANSWER---\")\n",
        "    num_steps = state['num_steps']\n",
        "    num_steps += 1\n",
        "\n",
        "    # Extract the final answer\n",
        "    llm2_response = state[\"llm2_response\"]\n",
        "    json_response = json.loads(llm2_response)\n",
        "    answer = json_response['Answer']\n",
        "\n",
        "    print(\"Final answer:\", answer)\n",
        "\n",
        "    # Save the answer to a file\n",
        "    write_markdown_file(str(answer), \"final_answer\")\n",
        "\n",
        "    # Update the state\n",
        "    return {\"final_answer\": answer, \"num_steps\":num_steps}"
      ],
      "metadata": {
        "id": "Wh1oJMBac2D6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_the_state_6(state):\n",
        "\n",
        "    \"\"\"\n",
        "    print the state\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---STATE PRINTER---\")\n",
        "    print(f\"Last llm response: {state['llm2_response']} \\n\" )\n",
        "    print(f\"Final answer: {state['final_answer']} \\n\" )\n",
        "    print(f\"Num steps: {state['num_steps']} \\n\")\n",
        "\n",
        "    return\n"
      ],
      "metadata": {
        "id": "tqLuANkiFUBA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the state\n",
        "\n",
        "The functions take the state as input.<br>\n",
        "The functions usually return a dict that automatically updates the state.<br>\n",
        "Therefore, the variables in the state need to correspond to variables that the functions output."
      ],
      "metadata": {
        "id": "4Yo-2d-KIf-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langgraph.graph import END, StateGraph"
      ],
      "metadata": {
        "id": "8bjIb61VIhFb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "### State\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the graph.\n",
        "\n",
        "    Attributes:\n",
        "        system_message: the ReAct system message\n",
        "        user_query: the question from the user\n",
        "        message_history: list of llm chat messages\n",
        "        llm2_response: the latest llm response\n",
        "        final_answer: final llm answer to the user query\n",
        "        num_steps: number of steps\n",
        "    \"\"\"\n",
        "\n",
        "    system_message : str\n",
        "    user_query : str\n",
        "    message_history : List[dict]\n",
        "    llm2_response : str\n",
        "    final_answer : str\n",
        "    num_steps : int\n"
      ],
      "metadata": {
        "id": "DLiAcneOIm4T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DL9aNOUKIm1X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the graph\n",
        "\n",
        "1. initialize_message_history_1 (node)\n",
        "2. query_llm_2 (node) (llm)\n",
        "3. route_to_websearch_or_answer_3 (cond edge)\n",
        "4. run_web_search_4 (node) (tavily)\n",
        "5. save_final_answer_5 (node)\n",
        "6. print_the_state_6 (node)"
      ],
      "metadata": {
        "id": "7Fz5eK9WRWAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the graph"
      ],
      "metadata": {
        "id": "QIe_sWG87Cok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(GraphState)"
      ],
      "metadata": {
        "id": "Lx0UOcxXQK6r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the nodes"
      ],
      "metadata": {
        "id": "fZRr0ubG66jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_node(\"initialize_message_history_1\", initialize_message_history_1)\n",
        "workflow.add_node(\"query_llm_2\", query_llm_2)\n",
        "workflow.add_node(\"run_web_search_4\", run_web_search_4)\n",
        "workflow.add_node(\"save_final_answer_5\", save_final_answer_5)\n",
        "workflow.add_node(\"print_the_state_6\", print_the_state_6)"
      ],
      "metadata": {
        "id": "4sj-cFCOQK4O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the edges"
      ],
      "metadata": {
        "id": "lB1QItos8Gv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INPUT\n",
        "\n",
        "# e-0\n",
        "workflow.set_entry_point(\"initialize_message_history_1\")\n",
        "\n",
        "# e-1\n",
        "workflow.add_edge(\"initialize_message_history_1\", \"query_llm_2\")\n",
        "\n",
        "# e-2\n",
        "workflow.add_conditional_edges(\n",
        "    \"query_llm_2\",\n",
        "    route_to_websearch_or_answer_3,\n",
        "    {\n",
        "        \"to_web_search\": \"run_web_search_4\", # e-3\n",
        "        \"to_final_answer\": \"save_final_answer_5\", # e-5\n",
        "    },\n",
        ")\n",
        "# e-4\n",
        "workflow.add_edge(\"run_web_search_4\", \"query_llm_2\")\n",
        "\n",
        "# e-6\n",
        "workflow.add_edge(\"save_final_answer_5\", \"print_the_state_6\")\n",
        "\n",
        "# e-7\n",
        "workflow.add_edge(\"print_the_state_6\", END)\n",
        "\n",
        "# END"
      ],
      "metadata": {
        "id": "UyFNQgD-MDrC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the graph"
      ],
      "metadata": {
        "id": "Z5g9uZKLBGGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "Tfi2e7ICQKzU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the system"
      ],
      "metadata": {
        "id": "aV9i8_QuBhMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What are the currencies of South Africa, Thailand and Indonesia?\"\n",
        "\n",
        "#user_query = \" Which actors won the best actress oscar in 2000 and 2001?\"\n",
        "\n",
        "inputs = {\"system_message\": system_message, \"user_query\": user_query, \"num_steps\": 0}"
      ],
      "metadata": {
        "id": "Vmv6gJGRQKwj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The printed outputs will be displayed when this cell is run\n",
        "\n",
        "output = app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqh3Ye3eB8mr",
        "outputId": "2a3d2152-8353-433a-946a-95d406ec4589"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---INITIALIZE MESSAGE HISTORY---\n",
            "Added system message.\n",
            "Added user query.\n",
            "Message history initialized.\n",
            "---MAKE LLM API CALL---\n",
            "<class 'list'>\n",
            "Response: {\n",
            "    \"Thought\": \"I need to find the currencies of three countries. I'll start with South Africa.\",\n",
            "    \"Action\": \"run_web_search: Currency of South Africa\",\n",
            "    \"Status\": \"PAUSE\"\n",
            "}\n",
            "Updated message history - assistant\n",
            "---ROUTE TO WEB SEARCH OR ANSWER---\n",
            "Status: PAUSE\n",
            "Routing to web search.\n",
            "---RUNNING WEB SEARCH---\n",
            "Search text: Currency of South Africa\n",
            "Updated message history - user, search results\n",
            "---MAKE LLM API CALL---\n",
            "<class 'list'>\n",
            "Response: {\n",
            "    \"Thought\": \"I have the information about the currency of South Africa. It's the South African rand (ZAR). Now, I need to find the currencies of Thailand and Indonesia.\",\n",
            "    \"Action\": \"run_web_search: Currency of Thailand\",\n",
            "    \"Status\": \"PAUSE\"\n",
            "}\n",
            "Updated message history - assistant\n",
            "---ROUTE TO WEB SEARCH OR ANSWER---\n",
            "Status: PAUSE\n",
            "Routing to web search.\n",
            "---RUNNING WEB SEARCH---\n",
            "Search text: Currency of Thailand\n",
            "Updated message history - user, search results\n",
            "---MAKE LLM API CALL---\n",
            "<class 'list'>\n",
            "Response: {\n",
            "    \"Thought\": \"I have the information about the currency of Thailand. It's the Thai baht (THB). Now, I need to find the currency of Indonesia.\",\n",
            "    \"Action\": \"run_web_search: Currency of Indonesia\",\n",
            "    \"Status\": \"PAUSE\"\n",
            "}\n",
            "Updated message history - assistant\n",
            "---ROUTE TO WEB SEARCH OR ANSWER---\n",
            "Status: PAUSE\n",
            "Routing to web search.\n",
            "---RUNNING WEB SEARCH---\n",
            "Search text: Currency of Indonesia\n",
            "Updated message history - user, search results\n",
            "---MAKE LLM API CALL---\n",
            "<class 'list'>\n",
            "Response: {\n",
            "    \"Thought\": \"I have all the information I need to answer the question. The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia.\",\n",
            "    \"Answer\": \"The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia.\",\n",
            "    \"Status\": \"DONE\"\n",
            "}\n",
            "Updated message history - assistant\n",
            "---ROUTE TO WEB SEARCH OR ANSWER---\n",
            "Status: DONE\n",
            "Routing to final amswer\n",
            "---EXTRACTING AND SAVING FINAL ANSWER---\n",
            "Final answer: The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia.\n",
            "---STATE PRINTER---\n",
            "Last llm response: {\n",
            "    \"Thought\": \"I have all the information I need to answer the question. The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia.\",\n",
            "    \"Answer\": \"The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia.\",\n",
            "    \"Status\": \"DONE\"\n",
            "} \n",
            "\n",
            "Final answer: The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia. \n",
            "\n",
            "Num steps: 9 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the final email\n",
        "\n",
        "print(output['final_answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAHcVEZaB8iz",
        "outputId": "d3519fad-40c1-4664-eeb6-536d1d2564f6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehiIYBt1SGTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the markdown file has been created\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "_fe6qoK7B8bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559ebf72-85ef-4242-f791-c7b6e8d25d56"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_answer.md  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the contents of the file\n",
        "\n",
        "!cat final_answer.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jicY5XvZaoCB",
        "outputId": "36034e29-563a-427a-daa5-b0263a685e40"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The currencies are: South African rand (ZAR) for South Africa, Thai baht (THB) for Thailand, and Indonesian rupiah (IDR) for Indonesia."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6CEn3pXa0ls"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}