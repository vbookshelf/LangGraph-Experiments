{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8701001,"sourceType":"datasetVersion","datasetId":612177},{"sourceId":164070052,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Ref:\n# YouTube Video\n# Sam Witteveen\n# Creating an AI Agent with LangGraph Llama 3 & Groq\n# https://www.youtube.com/watch?v=lvQ96Ssesfk","metadata":{"id":"gsE5rhFuCiQj","execution":{"iopub.status.busy":"2024-06-17T11:46:04.295523Z","iopub.execute_input":"2024-06-17T11:46:04.295910Z","iopub.status.idle":"2024-06-17T11:46:04.334090Z","shell.execute_reply.started":"2024-06-17T11:46:04.295880Z","shell.execute_reply":"2024-06-17T11:46:04.332624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U -q google-generativeai # Install the Python SDK\n#!pip -q install groq\n!pip -q install tavily-python","metadata":{"id":"GI-8V6WVCpC8","outputId":"a8ad750b-6be0-4f2e-e0ef-36989fbccd90","execution":{"iopub.status.busy":"2024-06-17T11:46:04.336513Z","iopub.execute_input":"2024-06-17T11:46:04.338126Z","iopub.status.idle":"2024-06-17T11:46:34.187428Z","shell.execute_reply.started":"2024-06-17T11:46:04.338070Z","shell.execute_reply":"2024-06-17T11:46:34.185182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install -U sentence-transformers\n!pip -q install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:46:34.190344Z","iopub.execute_input":"2024-06-17T11:46:34.190944Z","iopub.status.idle":"2024-06-17T11:47:06.445430Z","shell.execute_reply.started":"2024-06-17T11:46:34.190896Z","shell.execute_reply":"2024-06-17T11:47:06.443199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport re\nimport google.generativeai as genai\n#from google.colab import userdata","metadata":{"id":"EdY2JYCQePWH","execution":{"iopub.status.busy":"2024-06-17T11:47:06.448955Z","iopub.execute_input":"2024-06-17T11:47:06.449389Z","iopub.status.idle":"2024-06-17T11:47:08.466186Z","shell.execute_reply.started":"2024-06-17T11:47:06.449353Z","shell.execute_reply":"2024-06-17T11:47:08.464474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The embeddings and the dataframe created and saved in Part 1\n\nPATH_TO_EMBEDS = '../input/part-1-build-an-arxiv-rag-search-system-w-faiss/compressed_array.npz'\nPATH_TO_DF = '../input/part-1-build-an-arxiv-rag-search-system-w-faiss/compressed_dataframe.csv.gz'","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:47:08.467930Z","iopub.execute_input":"2024-06-17T11:47:08.468456Z","iopub.status.idle":"2024-06-17T11:47:08.474494Z","shell.execute_reply.started":"2024-06-17T11:47:08.468423Z","shell.execute_reply":"2024-06-17T11:47:08.473229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the API clients","metadata":{"id":"Cqxrj_T6C_z1"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:47:08.476304Z","iopub.execute_input":"2024-06-17T11:47:08.476803Z","iopub.status.idle":"2024-06-17T11:47:08.498486Z","shell.execute_reply.started":"2024-06-17T11:47:08.476763Z","shell.execute_reply":"2024-06-17T11:47:08.495899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tavily import TavilyClient\n\ntavily_client = TavilyClient(api_key = user_secrets.get_secret(\"TAVILY_API_KEY\"))","metadata":{"id":"PzAyFOnWC_d9","execution":{"iopub.status.busy":"2024-06-17T11:47:45.638319Z","iopub.execute_input":"2024-06-17T11:47:45.640021Z","iopub.status.idle":"2024-06-17T11:47:46.160924Z","shell.execute_reply.started":"2024-06-17T11:47:45.639958Z","shell.execute_reply":"2024-06-17T11:47:46.159049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\"))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:47:47.362592Z","iopub.execute_input":"2024-06-17T11:47:47.363350Z","iopub.status.idle":"2024-06-17T11:47:47.451520Z","shell.execute_reply.started":"2024-06-17T11:47:47.363315Z","shell.execute_reply":"2024-06-17T11:47:47.450149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What is the objective?\n\n- Create an llm research assistant that has a chat interface\n","metadata":{"id":"2Ox3Pq34Eq8I"}},{"cell_type":"code","source":"","metadata":{"id":"eXpFe86y6DOF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a list of agents\n\nAGENTS\n1. chat_agent\n2. router_agent\n3. research_agent\n4. final_answer_agent\n\nTOOLS\n- Function that returns an avaerage dog weight given a dog breed\n- Calculate function that returns answers to calculations\n\nBLOCKS\n- ReAct block","metadata":{"id":"33Z_T0CgFYif"}},{"cell_type":"markdown","source":"# Helper functions","metadata":{"id":"LwW76J3yJ5QN"}},{"cell_type":"code","source":"def write_markdown_file(content, filename):\n  \"\"\"Writes the given content as a markdown file to the local directory.\n\n  Args:\n    content: The string content to write to the file.\n    filename: The filename to save the file as.\n  \"\"\"\n  with open(f\"{filename}.md\", \"w\") as f:\n    f.write(content)\n","metadata":{"id":"W3YQhpBeJ69J","execution":{"iopub.status.busy":"2024-06-17T11:47:52.482025Z","iopub.execute_input":"2024-06-17T11:47:52.482533Z","iopub.status.idle":"2024-06-17T11:47:52.491152Z","shell.execute_reply.started":"2024-06-17T11:47:52.482495Z","shell.execute_reply":"2024-06-17T11:47:52.487983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_message_history(system_message, user_input):\n\n    \"\"\"\n    Create a message history messages list.\n    Args:\n        system_message (str): The system message\n        user_query (str): The user input\n    Returns:\n        A list of dicts in OpenAi chat format\n    \"\"\"\n\n    message_history = [\n                        {\n                            \"role\": \"system\",\n                            \"content\": system_message\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": user_input\n                        }\n                    ]\n\n    return message_history\n\n","metadata":{"id":"1pl8ksbbeuSD","execution":{"iopub.status.busy":"2024-06-17T11:47:53.033954Z","iopub.execute_input":"2024-06-17T11:47:53.034353Z","iopub.status.idle":"2024-06-17T11:47:53.041567Z","shell.execute_reply.started":"2024-06-17T11:47:53.034323Z","shell.execute_reply":"2024-06-17T11:47:53.040254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_message_history(system_message):\n\n    \"\"\"\n    Create a message history messages list.\n    Args:\n        system_message (str): The system message\n        user_query (str): The user input\n    Returns:\n        A list of dicts in OpenAi chat format\n    \"\"\"\n\n    message_history = [\n                        {\n                            \"role\": \"system\",\n                            \"content\": system_message\n                        }\n                    ]\n\n    return message_history\n\n","metadata":{"id":"ywzSfgqFFOkZ","execution":{"iopub.status.busy":"2024-06-17T11:47:53.466576Z","iopub.execute_input":"2024-06-17T11:47:53.467106Z","iopub.status.idle":"2024-06-17T11:47:53.473689Z","shell.execute_reply.started":"2024-06-17T11:47:53.467070Z","shell.execute_reply":"2024-06-17T11:47:53.472532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## System messages","metadata":{}},{"cell_type":"code","source":"comp_description = \"\"\"\n\nCompetition Name: AI Mathematical Olympiad - Progress Prize 1\n\nThe goal of this competition is to create algorithms and models that can solve tricky math problems written in LaTeX format. Your participation will help to advance AI models’ mathematical reasoning skills and drive frontier knowledge.\n\nThe ability to reason mathematically is a critical milestone for AI. Mathematical reasoning is the foundation for solving many complex problems, from engineering marvels to intricate financial models. However, current AI capabilities are limited in this area.\n\nThe AI Mathematical Olympiad (AIMO) Prize is a new $10mn prize fund to spur the open development of AI models capable of performing as well as top human participants in the International Mathematical Olympiad (IMO).\nThis competition includes 110 problems similar to an intermediate-level high school math challenge. The Gemma 7B benchmark for these problems is 3/50 on the public and private test sets.\n\nThe assessment of AI models' mathematical reasoning skills faces a significant hurdle, the issue of train-test leakage. Models trained on Internet-scale datasets may inadvertently encounter test questions during training, skewing the evaluation process.\n\nTo address this challenge, this competition uses a dataset of 110 novel math problems, created by an international team of problem solvers, recognizing the need for a transparent and fair evaluation framework. The dataset encompasses a range of difficulty levels, from simple arithmetic to algebraic thinking and geometric reasoning. This will help to strengthen the benchmarks for assessing AI models' mathematical reasoning skills, without the risk of contamination from training data.\n\nThis competition offers an exciting opportunity to benchmark open AI models against each other and foster healthy competition and innovation in the field. By addressing this initial benchmarking problem, you will contribute to advancing AI capabilities and help to ensure that its potential benefits outweigh the risks.\n\nJoin us as we work towards a future where AI models’ mathematical reasoning skills are accurately and reliably assessed, driving progress and innovation across industries.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:47:54.740628Z","iopub.execute_input":"2024-06-17T11:47:54.741330Z","iopub.status.idle":"2024-06-17T11:47:54.748226Z","shell.execute_reply.started":"2024-06-17T11:47:54.741281Z","shell.execute_reply":"2024-06-17T11:47:54.747070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat_agent_system_message = f\"\"\"\nYou are a helpful Kaggle competition research assistant.\nCompetition: {comp_description}\n\n1. You provide polite answers to simple questions.\nIf the user's input requires only a simple answer, then output your answer as JSON.\n\nExample session:\n\nQuestion: Hello. How are you?\n\nYou output:\n\n{{\n\"Answer\": \"I'm fine, thanks.\",\n\"Status\": \"DONE\"\n}}\n\n2. You can also run in a loop of Thought, Action, PAUSE, Observation.\nAt the end of the loop, you output an Answer.\nUse Thought to describe your thoughts about the question you have been asked.\nUse Action to run one of the actions available to you - then return PAUSE.\nObservation will be the result of running those actions.\nOutput your response as a JSON string.\n\nYour available actions are:\n\nfind_arxiv_research_papers:\ne.g. find_arxiv_research_papers: [list of search keywords and phrases for a RAG search of the ArXiv database.]\nReturns research papers from the ArXiv database.\n\nrun_web_search:\ne.g. run_web_search: [list of search keywords and phrases for a web search]\nReturns text content from search results.\n\nYou can only call one action at a time.\n\nExample session:\n\nQuestion: What are the latest techniques for detecting Pneumonia on x-rays using AI?\n{{\n\"Thought\": \"I should look for relevant research papers in the ArXiv database by using find_arxiv_research_papers.\",\n\"Action\": {{\"function\":\"find_arxiv_research_papers\", \"input\": [\"Pneumonia detection with AI\", \"Computer vision\", \"Object detection\"]}},\n\"Status\": \"PAUSE\"\n}}\n\nYou will be called again with this:\n\nObservation: <results>A list of research papers and their content</results>\n\nYou then output:\n{{\n\"Answer\": \"Your final report.\",\n\"Status\": \"DONE\"\n}}\n\"\"\".strip()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:47:55.349393Z","iopub.execute_input":"2024-06-17T11:47:55.350008Z","iopub.status.idle":"2024-06-17T11:47:55.358456Z","shell.execute_reply.started":"2024-06-17T11:47:55.349953Z","shell.execute_reply":"2024-06-17T11:47:55.357086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up the LLM","metadata":{"id":"0reljPhbezOM"}},{"cell_type":"code","source":"def make_llm_api_call(message):\n\n    \"\"\"\n    Makes a call to the Llama3 model on Groq.\n    Args:\n        message_history (List of dicts): The message history\n    Returns:\n        response_text: (str): The text response from the LLM\n    \"\"\"\n\n    response = chat.send_message(message)\n    \n    text_response = response.text\n\n    return text_response\n\n\n# Example\n\nmodel = genai.GenerativeModel(\n    \"models/gemini-1.5-flash\",\n    system_instruction=\"You are a helpful assistant.\",\n)\n\nchat = model.start_chat()\n\nuser_message = \"What's your name?\"\n\nresponse = make_llm_api_call(user_message)\n\nprint(response)","metadata":{"id":"eZ-R0vrQezl2","outputId":"5803cc30-16b1-4ccf-bc2f-cb4a6e8dd8bb","execution":{"iopub.status.busy":"2024-06-17T11:51:02.032734Z","iopub.execute_input":"2024-06-17T11:51:02.033129Z","iopub.status.idle":"2024-06-17T11:51:02.884788Z","shell.execute_reply.started":"2024-06-17T11:51:02.033101Z","shell.execute_reply":"2024-06-17T11:51:02.883536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up the tools\n\n","metadata":{"id":"ShdYyu_Ne7R2"}},{"cell_type":"markdown","source":"### ArXiv RAG search tool","metadata":{}},{"cell_type":"code","source":"def run_faiss_search(query_text, top_k):\n    \n    # Run FAISS exhaustive search\n    \n    query = [query_text]\n\n    # Vectorize the query string\n    query_embedding = sent_model.encode(query)\n\n    # Run the query\n    # index_vals refers to the chunk_list index values\n    scores, index_vals = faiss_index.search(query_embedding, top_k)\n    \n    # Get the list of index vals\n    index_vals_list = index_vals[0]\n    \n    return index_vals_list\n    \n\ndef run_rerank(index_vals_list, query_text):\n    \n    chunk_list = list(df_data['prepared_text'])\n\n    # Replace the chunk index values with the corresponding strings\n    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n\n    # Format the input for the cross encoder\n    # The input to the cross_encoder is a list of lists\n    # [[query_text, pred_text1], [query_text, pred_text2], ...]\n\n    cross_input_list = []\n\n    for item in pred_strings_list:\n\n        new_list = [query_text, item]\n\n        cross_input_list.append(new_list)\n\n\n    # Put the pred text into a dataframe\n    df = pd.DataFrame(cross_input_list, columns=['query_text', 'pred_text'])\n\n    # Save the orginal index (i.e. df_data index values)\n    df['original_index'] = index_vals_list\n\n    # Now, score all retrieved passages using the cross_encoder\n    cross_scores = cross_encoder.predict(cross_input_list)\n\n    # Add the scores to the dataframe\n    df['cross_scores'] = cross_scores\n\n    # Sort the DataFrame in descending order based on the scores\n    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n    \n    # Reset the index (*This was missed previously*)\n    df_sorted = df_sorted.reset_index(drop=True)\n\n    pred_list = []\n\n    for i in range(0,len(df_sorted)):\n\n        text = df_sorted.loc[i, 'pred_text']\n\n        # Get the arxiv id\n        # original_index refers to the index values in df_filtered\n        original_index = df_sorted.loc[i, 'original_index']\n        arxiv_id = df_data.loc[original_index, 'id']\n        cat_text = df_data.loc[original_index, 'cat_text']\n        title = df_data.loc[original_index, 'title']\n\n        # Crete the link to the research paper pdf\n        link_to_pdf = f'https://arxiv.org/pdf/{arxiv_id}'\n\n        item = {\n            'arxiv_id': arxiv_id,\n            'link_to_pdf': link_to_pdf,\n            'cat_text': cat_text,\n            'title': title,\n            'abstract': text\n        }\n\n        pred_list.append(item)\n\n    return pred_list\n\n\ndef print_search_results(pred_list, num_results_to_print):\n    \n    for i in range(0,num_results_to_print):\n        \n        pred_dict = pred_list[i]\n        \n        link_to_pdf = pred_dict['link_to_pdf']\n        abstract = pred_dict['abstract']\n        cat_text = pred_dict['cat_text']\n        title = pred_dict['title']\n\n        print('Title:',title)\n        print('Categories:',cat_text)\n        print('Abstract:',abstract)\n        print('Link to pdf:',link_to_pdf)\n        print()\n    \n   \ndef run_arxiv_search(query_text, top_k=50):\n    \n    # Run a faiss greedy search\n    pred_index_list = run_faiss_search(query_text, top_k)\n\n    # This returns a list of dicts with length equal to top_k\n    pred_list = run_rerank(pred_index_list, query_text)\n    \n    # Print the results\n    #print_search_results(pred_list, num_results_to_print)\n    \n    return pred_list\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-17T12:22:57.401532Z","iopub.execute_input":"2024-06-17T12:22:57.402077Z","iopub.status.idle":"2024-06-17T12:22:57.421496Z","shell.execute_reply.started":"2024-06-17T12:22:57.402036Z","shell.execute_reply":"2024-06-17T12:22:57.419802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the compressed array\nembeddings = np.load(PATH_TO_EMBEDS)\n\n# Access the array by the name you specified ('my_array' in this case)\nembeddings = embeddings['array_data']\n\nembeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:51:48.628191Z","iopub.execute_input":"2024-06-17T11:51:48.628569Z","iopub.status.idle":"2024-06-17T11:52:42.651523Z","shell.execute_reply.started":"2024-06-17T11:51:48.628540Z","shell.execute_reply":"2024-06-17T11:52:42.649876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the compressed DataFrame\n\ndf_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n\nprint(df_data.shape)\n\n#df_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:52:42.653548Z","iopub.execute_input":"2024-06-17T11:52:42.653943Z","iopub.status.idle":"2024-06-17T11:54:15.780467Z","shell.execute_reply.started":"2024-06-17T11:52:42.653913Z","shell.execute_reply":"2024-06-17T11:54:15.779083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize FAISS\n\nimport faiss\n\nembed_length = embeddings.shape[1]\n\nfaiss_index = faiss.IndexFlatL2(embed_length)\n\n# Add the embeddings to the index\nfaiss_index.add(embeddings)\n\nfaiss_index.is_trained","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:54:15.781871Z","iopub.execute_input":"2024-06-17T11:54:15.782197Z","iopub.status.idle":"2024-06-17T11:54:20.097570Z","shell.execute_reply.started":"2024-06-17T11:54:15.782169Z","shell.execute_reply":"2024-06-17T11:54:20.096146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize sentence_transformers\n\nfrom sentence_transformers import SentenceTransformer\n\nsent_model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-17T12:10:56.532907Z","iopub.execute_input":"2024-06-17T12:10:56.533337Z","iopub.status.idle":"2024-06-17T12:10:57.092196Z","shell.execute_reply.started":"2024-06-17T12:10:56.533307Z","shell.execute_reply":"2024-06-17T12:10:57.090988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the cross_encoder for reranking\n\nfrom sentence_transformers import CrossEncoder\n\n# We use a cross-encoder, to re-rank the results list to improve the quality\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-17T11:54:44.945424Z","iopub.execute_input":"2024-06-17T11:54:44.946071Z","iopub.status.idle":"2024-06-17T11:54:46.989309Z","shell.execute_reply.started":"2024-06-17T11:54:44.946038Z","shell.execute_reply":"2024-06-17T11:54:46.987258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example\n\nquery_text = \"\"\"\n\nI want to build an invisibility cloak like the one in Harry Potter.\n\n\"\"\"\n\n\n# RUN THE SEARCH\nnum_results_to_print = 20 # top_k = 300\npred_list = run_arxiv_search(query_text, top_k=5)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-17T11:54:46.990775Z","iopub.execute_input":"2024-06-17T11:54:46.991195Z","iopub.status.idle":"2024-06-17T11:54:49.858075Z","shell.execute_reply.started":"2024-06-17T11:54:46.991162Z","shell.execute_reply":"2024-06-17T11:54:49.856919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tavily web search","metadata":{}},{"cell_type":"code","source":"def run_tavily_search(query, num_results=50):\n\n    \"\"\"\n    Uses the Tavily API to run a web search\n    Args:\n        query (str): The user query\n        num_results (int): Num search results\n    Returns:\n        tav_response (json string): The search results in json format\n    \"\"\"\n\n    # For basic search:\n    tav_response = tavily_client.search(query=query, max_results=num_results)\n\n    return tav_response\n\n\n# Example\n\nquery = \"How much does a bulldog weigh?\"\n\nresults = run_tavily_search(query, num_results=2)\n\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T12:23:12.462245Z","iopub.execute_input":"2024-06-17T12:23:12.462660Z","iopub.status.idle":"2024-06-17T12:23:13.841566Z","shell.execute_reply.started":"2024-06-17T12:23:12.462631Z","shell.execute_reply":"2024-06-17T12:23:13.840349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up the Agents","metadata":{"id":"kGZoKXk4fDX4"}},{"cell_type":"code","source":"def run_chat_agent(message):\n\n    print(\"---CHAT AGENT---\")\n\n    # Prompt the llm\n    response = make_llm_api_call(message)\n\n    print(response)\n\n    return response\n\n\n\n# Example\n\nmodel = genai.GenerativeModel(\n    \"models/gemini-1.5-flash\",\n    system_instruction = chat_agent_system_message,\n)\n\nchat = model.start_chat()\n\nmessage = \"hello\"\n\n# Prompt the chat_agent\nresponse = run_chat_agent(message)\n\nresponse","metadata":{"id":"c67LzbXufDJc","outputId":"04326aff-76d5-414c-9f27-32168ece3f7a","execution":{"iopub.status.busy":"2024-06-17T11:55:45.609561Z","iopub.execute_input":"2024-06-17T11:55:45.609986Z","iopub.status.idle":"2024-06-17T11:55:47.149979Z","shell.execute_reply.started":"2024-06-17T11:55:45.609954Z","shell.execute_reply":"2024-06-17T11:55:47.148502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_router_agent(llm_response):\n\n    \"\"\"\n    Route to web search or not.\n    Args:\n        state (dict): The current graph state\n    Returns:\n        str: Next node to call\n    \"\"\"\n\n    print(\"---ROUTER AGENT---\")\n\n\n    # Extract the status\n    json_response = json.loads(llm_response)\n    status = json_response['Status']\n    #status = extract_json_str_value(response, \"Status\").strip()\n\n    print(\"Status:\", status)\n\n    if status == 'PAUSE':\n        print(\"Route: to_research_agent\")\n        return \"to_research_agent\"\n\n    elif status == 'DONE':\n        print(\"Route: to_final_answer\")\n        return \"to_final_answer\"\n\n\n\n# Example\n\nmodel = genai.GenerativeModel(\n    \"models/gemini-1.5-flash\",\n    system_instruction = chat_agent_system_message,\n)\n\nchat = model.start_chat()\n\nmessage = \"hello\"\n\n# Prompt the chat_agent\nresponse = run_chat_agent(message)\n\n# Run router_agent\nroute = run_router_agent(response)\n\nroute","metadata":{"id":"NHOvD12NfDHX","outputId":"25d189dc-9de0-4fcc-c198-833ccbd1e470","execution":{"iopub.status.busy":"2024-06-17T11:57:04.429654Z","iopub.execute_input":"2024-06-17T11:57:04.430482Z","iopub.status.idle":"2024-06-17T11:57:05.336154Z","shell.execute_reply.started":"2024-06-17T11:57:04.430448Z","shell.execute_reply":"2024-06-17T11:57:05.334795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_research_agent(llm_response):\n\n    print(\"---RESEARCH AGENT---\")\n\n    # Extract the status\n    json_response = json.loads(llm_response)\n    action_dict = json_response['Action']\n    func_to_run = action_dict['function']\n    func_input_list = action_dict['input']\n    \n    answer_list = []\n\n    if func_to_run == \"find_arxiv_research_papers\":\n        for search_query in func_input_list:\n            answer = run_arxiv_search(search_query, top_k=50)\n            answer_list.append(answer)\n    else:\n        for search_query in func_input_list:\n            answer = run_tavily_search(search_query, num_results=50)\n            answer_list.append(answer)\n\n    print(\"func_to_run:\", func_to_run)\n    print(\"func_arg:\", func_input_list)\n    print(\"Output:\", answer_list)\n\n    return answer_list\n\n\n\n# Example\n\nmodel = genai.GenerativeModel(\n    \"models/gemini-1.5-flash\",\n    system_instruction = chat_agent_system_message,\n)\n\nchat = model.start_chat()\n\nmessage = \"What are some of the state of the art techniques that I can use to solve this problem?\"\n\n\n# Prompt the chat_agent\nresponse = run_chat_agent(message)\n\n# Run router_agent\nroute = run_router_agent(response)\n\n\nif route == \"to_research_agent\":\n    answer = run_research_agent(response)\n\n    # Update message history\n    #message = {\"role\": \"user\", \"content\": f\"Observation: {answer}\"}\n    #message_history.append(message)\n\n","metadata":{"id":"Zj3CJ1q_nGsz","outputId":"dd48f886-b14b-43be-a08d-dd94537ed881","execution":{"iopub.status.busy":"2024-06-17T12:26:30.241855Z","iopub.execute_input":"2024-06-17T12:26:30.242231Z","iopub.status.idle":"2024-06-17T12:26:33.028027Z","shell.execute_reply.started":"2024-06-17T12:26:30.242203Z","shell.execute_reply":"2024-06-17T12:26:33.026927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_final_answer_agent(llm_response):\n\n    print(\"---FINAL ANSWER AGENT---\")\n\n    json_response = json.loads(llm_response)\n    final_answer = json_response['Answer']\n\n    print(\"Final answer:\", final_answer)","metadata":{"id":"Xrmd29mU7ZLa","execution":{"iopub.status.busy":"2024-06-17T12:02:25.641480Z","iopub.execute_input":"2024-06-17T12:02:25.641967Z","iopub.status.idle":"2024-06-17T12:02:25.649734Z","shell.execute_reply.started":"2024-06-17T12:02:25.641933Z","shell.execute_reply":"2024-06-17T12:02:25.646807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run the system","metadata":{"id":"l6enpJPmzLim"}},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    \"models/gemini-1.5-flash\",\n    system_instruction = chat_agent_system_message,\n)\n\nchat = model.start_chat()\n\n\nmessage = \"What is the current state of the art for this subject?\"\n\nfor i in range(0,10):\n\n    # Prompt the chat_agent\n    llm_response = run_chat_agent(message)\n\n    # Run router_agent\n    route = run_router_agent(llm_response)\n\n\n    if route == \"to_research_agent\":\n\n        answer = run_research_agent(llm_response)\n        \n        user_input = f\"Observation: {answer}\"\n\n    else:\n\n        run_final_answer_agent(llm_response)\n\n        break\n\n","metadata":{"id":"lSuoHVFDy8WZ","outputId":"80272366-f203-4680-f6b0-7f4562f46626","execution":{"iopub.status.busy":"2024-06-17T12:15:07.853049Z","iopub.execute_input":"2024-06-17T12:15:07.853486Z","iopub.status.idle":"2024-06-17T12:15:09.096768Z","shell.execute_reply.started":"2024-06-17T12:15:07.853454Z","shell.execute_reply":"2024-06-17T12:15:09.095339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run infinitely with a user input field","metadata":{"id":"kN_-LCvuNrE_"}},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    \"models/gemini-1.5-flash\",\n    system_instruction = chat_agent_system_message,\n)\n\nchat = model.start_chat()\n\n\nwhile True:\n\n    print()\n    print(\"==========\")\n    user_input = input(\"Enter something (or 'q' to quit): \")\n    print(\"==========\")\n    \n    i = i + 1\n\n    if user_input.lower() == 'q':\n        print(\"Exiting the loop. Goodbye!\")\n        break  # Exit the loop\n\n\n    for i in range(0,10):\n\n        # Prompt the chat_agent\n        llm_response = run_chat_agent(user_input)\n\n        # Run router_agent\n        route = run_router_agent(llm_response)\n\n\n        if route == \"to_research_agent\":\n\n            answer = run_research_agent(llm_response)\n            \n            user_input = f\"Observation: {answer}\"\n\n        else:\n\n            run_final_answer_agent(llm_response)\n\n            break\n\n","metadata":{"id":"fZLpFq4tvmG7","outputId":"e01afa16-4c1a-4029-a568-ae6838d2d096","execution":{"iopub.status.busy":"2024-06-17T12:39:05.438245Z","iopub.execute_input":"2024-06-17T12:39:05.438779Z","iopub.status.idle":"2024-06-17T12:58:09.147471Z","shell.execute_reply.started":"2024-06-17T12:39:05.438746Z","shell.execute_reply":"2024-06-17T12:58:09.146149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"pYqL1V1TJ_Yq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"5KB5ClUEJ_UL"},"execution_count":null,"outputs":[]}]}